---
title: "Multivariate Tree Analysis."
date: 01/05/2020
author: "By: Joshua Freimark"
output: github_document
---

* Applied Statistics and Econometrics - ECON 536

* R version 4.0.0 (2020-04-24) 


```{r, include=FALSE, message=FALSE, warning=FALSE}
library(readxl)
library(pastecs)
library(magrittr)
library(gridExtra)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(dplyr)
library(DT)
library(MASS)
library(leaps)
library(glmnet)
library(PerformanceAnalytics)
library(corrr)
library(tidyr)
library(readxl)
library(rpart)
library(rpart.plot)
library(caret)
library(tidyverse)
library(rpart)				       
library(rattle)					
library(rpart.plot)		
library(RColorBrewer)			
library(party)					
library(partykit)			
library(caret)	
library(olsrr)
library(FactoMineR)
library(factoextra)
library(vip)       
library(pdp)        
library(ipred)
library(tree)
library(randomForest)
```


### 1. Univariate Tree Analysis.

* Use the CART (Classification and regression tree) estimator on the median home value with the right hand side variable LSTAT. Report results for 7 terminal nodes. Provide a graph of the median home value as a function of LSTAT, and a graph of your piecewise approximation to it. 

```{r, message=FALSE, warning=FALSE}
Final536 = read_excel("C:/Users/Joshu/OneDrive/Desktop/WSU courses/Econ536/Homework/Final/Final536.xlsx")
```



```{r, include=TRUE, message=FALSE, warning=FALSE}
set.seed(123)
tree = rpart(medv ~ lstat, method = "anova", data = Final536)
tree
summary(tree)
rpart.plot(tree, type = 3, digits = 4, fallen.leaves = TRUE)

```


\pagebreak


* The rpart function cannot plot the piecewise estimation of median home price as a function of lstat so I must use the tree function for the dendrogram and the piecewise approximation plot. They both yield the same results. I will also plot the tree dendrogram to verify the results are the same as rpart.
```{r, include=TRUE, message=FALSE, warning=FALSE}
treeplot = tree(medv~lstat, Final536)
plot(treeplot)
text(treeplot,cex=0.75)
```

\pagebreak

* Graph of median home value as a function of lstat and a piecewise approximation to it.
```{r message=FALSE, warning=FALSE}
plot(Final536$lstat, Final536$medv, type="p", xlab="lstat", ylab="medv", main ="Median home value as a function of lstat")
partition.tree(treeplot, add = TRUE, cex = 1.5, col = "red", lwd = 2)
```













\pagebreak

### 2. Multivariate Tree Analysis.

* Partition the data into 80% training and 20% testing data. We will use the training data to train the model and the test data to validate the model. If we see little to no difference between the model performance on the testing and training data, then our model is validated.
```{r, include=TRUE, message=FALSE, warning=FALSE}
index.cart <- sample(nrow(Final536),nrow(Final536)*0.80)
train.cart <- Final536[index.cart,]
test.cart <- Final536[-index.cart,]
```




* Use all the variables for multivariate tree analysis Estimate the tree using the training data and all of the variables. 0.005 is the complexity param used for the full tree (tree1). The cp parameter is the minimum improvement required for the tree to make another split. The smaller the cp parameter, will result in a lower criterion needed for the tree to split, which will result in deeper tree with more splits and nodes. This tree should have many splits.
```{r}
tree1 = rpart(medv ~., method = "anova", data = train.cart, cp = 0.005)
tree1
#summary(tree1)
```



* This tree is overly complex and is five pages of summary output. This model is well overfit so let’s fix that and reduce the variance by pruning this tree.
```{r message=FALSE, warning=FALSE}
rpart.plot(tree1, type = 3, digits = 4, fallen.leaves = TRUE)
```
\pagebreak

* Check the model's performance for the un-pruned tree.
```{r message=FALSE, warning=FALSE}
train.pred.tree1 = predict(tree1, train.cart)
test.pred.tree1 = predict(tree1, test.cart)

MPSE1 = mean((test.pred.tree1 - test.cart$medv)^2)                     #MPSE
MSE1 = mean((train.pred.tree1 - train.cart$medv)^2)                    #MSE
MAE_func = function(actual, predicted) {mean(abs(actual - predicted))} #function to calculate MAE
MAE1 = MAE_func(test.cart$medv, test.pred.tree1)

tree1_not_pruned = c("tree1", MSE1, MPSE1, MAE1)
comparison_table = c("First tree model","MSE", "MSPE", "MAE")
data.frame(cbind(comparison_table, tree1_not_pruned ))
```

\pagebreak 

* We can see in this plot that the relative cross validation error is the lowest  around the 6th split (seen on the top axis). The plot also shows that even if we chose an exceedingly small cp value, there is diminishing benefit by doing so measured by the relative cross validation error. It is best to try to find a cp parameter that lowers the relative cross validation error with the least complex tree structure. From the summary output above we can find the cp value that is associated with 6th nsplit which is 0.017634191.
```{r message=FALSE, warning=FALSE}
plotcp(tree1)
```


\pagebreak




### 3. Prune the tree by altering the complexity parameter

* We will use 0.017634191 for the cp value associated with the 6th nsplit this time.
```{r message=FALSE, warning=FALSE}
tree2_prune = prune(tree1, cp = 0.017634191)
fancyRpartPlot(tree2_prune)
```


* Check the model performance. We can see the pruned tree performed worse than the first tree that was estimated that was not pruned. We measure the performance by the mean squared error (MSE), mean square prediction error (MSPE), and mean absolute error (MAE). 
```{r message=FALSE, warning=FALSE}
train.pred.tree2 = predict(tree2_prune, train.cart)
test.pred.tree2 = predict(tree2_prune, test.cart)
MPSE2 = mean((test.pred.tree2 - test.cart$medv)^2)                     #MPSE
MSE2 = mean((train.pred.tree2 - train.cart$medv)^2)                    #MSE
MAE_func = function(actual, predicted) {mean(abs(actual - predicted))} #function to calculate MAE
MAE2 = MAE_func(test.cart$medv, test.pred.tree2)

tree2.pruned = c("tree2_prune", MSE2, MPSE2, MAE2)
comparison_table = c("pruned regression tree","MSE", "MSPE", "MAE")
data.frame(cbind(comparison_table, tree2.pruned, tree1_not_pruned))
```



* Lets try to the cp parameter of 0.0132644 at nsplit 7.
```{r message=FALSE, warning=FALSE}
tree3_prune = prune(tree1, cp = 0.0132644) #from printcp(tree1): cp=0.0132644, nsplit=7 
```




* Check if our model using cp=0.0132644 improved from the previous model that used cp=0.017634191
```{r message=FALSE, warning=FALSE}
train.pred.tree3 = predict(tree3_prune, train.cart)
test.pred.tree3 = predict(tree3_prune, test.cart)
MPSE3 = mean((test.pred.tree3 - test.cart$medv)^2)          #MPSE
MSE3 = mean((train.pred.tree3 - train.cart$medv)^2)    #MSE
MAE_func = function(actual, predicted) {mean(abs(actual - predicted))} # function to calculate MAE
MAE3 = MAE_func(test.cart$medv, test.pred.tree3)

tree3.pruned = c("tree3_prune", MSE3, MPSE3, MAE3)
comparison_table = c("pruned regression tree","MSE", "MSPE", "MAE")
data.frame(cbind(comparison_table, tree3.pruned, tree2.pruned))
```


* Lastly, we will try cp=0.0077842 at nsplit 8 
```{r message=FALSE, warning=FALSE}
tree4_prune = prune(tree1, cp = 0.0077842) #from printcp(tree1): 0.0077842      8 
fancyRpartPlot(tree4_prune)
```

### 4. Compare all of the models. 
```{r message=FALSE, warning=FALSE}
train.pred.tree4 = predict(tree4_prune, train.cart)
test.pred.tree4 = predict(tree4_prune, test.cart)
MPSE4 = mean((test.pred.tree4 - test.cart$medv)^2)          #MPSE
MSE4 = mean((train.pred.tree4 - train.cart$medv)^2)    #MSE
MAE_func = function(actual, predicted) {mean(abs(actual - predicted))} # function to calculate MAE
MAE4 = MAE_func(test.cart$medv, test.pred.tree4)

tree4.pruned = c("tree4_prune", MSE4, MPSE4, MAE4)
comparison_table = c("pruned regression tree","MSE", "MSPE", "MAE")
data.frame(cbind(comparison_table, tree4.pruned, tree3.pruned, tree2.pruned, tree1_not_pruned))
```



\subsection{}
* We can see that altering the complexity parameter did not improve the model performance compared to the first tree we estimated. Among the pruned trees, the 4th has the lowest MSE, MSPE, and MAE and it is a bit simpler in terms of less splits and leave nodes then the un-pruned tree. 

\pagebreak

* We will use bagging to reduce the variance of a model with a combination of aggregation and bootstrapping (which is bagging). Bagging is a model averaging approach based off many new trees.


```{r message=FALSE, warning=FALSE}
formu <- as.formula("medv ~ .")
bag = randomForest(formu, data=train.cart, mtry=16, importance =TRUE)
bag
plot(bag)
```



\pagebreak


* We do not achieve very much benefit by growing more than 100 trees. Let’s just grow 150 to be safe.
```{r message=FALSE, warning=FALSE}
bag2 = randomForest(formu, data=train.cart, mtry=16, importance =TRUE, ntree = 150)
bag2
```


\ 
\ 
\   

* We used pruning and bagging to reduce the variance. Bagging is a good technique because it retains a low bias while also reducing variance. Pruning reduces the model from overfitting the data and overall, we can see that our model explained 88\% of the variation post-bagging. The MSE is 10.44 which is lower than the MSE of any of the previous trees. Overall, my pruning method was not the most efficient. Cross validation is one option that could be used for pruning. This method would choose cross-validated error that is the minimum, like my procedure above, but using cross-validation automates the process and reduced the human error. This would help to reduce the variance further. Our tree does contain some bias due to the large depth of the tree. Trees are sensitive to splits and the decision to split can have large impacts on the model results.

```{r message=FALSE, warning=FALSE}

```




```{r message=FALSE, warning=FALSE}

```

